{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNN)\n",
    "\n",
    "In this exercise, we will work with Recurrent Neural Networks (RNN). An RNN is a class of neural networks where the output not only depends on the current input but also on previous inputs along a given input sequence. This allows exhibiting temporal dynamic behavior and contextual information in a sequence. Common applications for RNN are:\n",
    "\n",
    "- time-series analysis\n",
    "- speech recognition\n",
    "- machine translation\n",
    "- image captioning\n",
    "- sentiment analysis\n",
    "\n",
    "\n",
    "## 0. Goal of this exercise\n",
    "\n",
    "This exercise notebook should help you to experiment with how recurrent neural networks are implemented. Therefore, this notebook is structured as follows:\n",
    "1. Check out our implementation of a simple RNN class in Pytorch.\n",
    "2. Explore the back-propagation of the gradients in the RNN and discuss the vanishing gradient problem.\n",
    "3. Implement your LSTM (Long-Short Term Memory) Network and show that this architecture improves the vanishing gradient problem.\n",
    "4. Learn how to deal with different length sequences in the same batch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Mount folder in Colab\n",
    "\n",
    "Uncomment the following cell to mount your gdrive if you are using the notebook in google colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following lines if you want to use Google Colab\n",
    "# We presume you created a folder \"i2dl\" within your main drive folder, and put the exercise there.\n",
    "# NOTE: terminate all other colab sessions that use GPU!\n",
    "# NOTE 2: Make sure the correct exercise folder (e.g exercise_11) is given.\n",
    "\n",
    "\"\"\"\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "gdrive_path='/content/gdrive/MyDrive/i2dl/exercise_11'\n",
    "\n",
    "# This will mount your google drive under 'MyDrive'\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "# In order to access the files in this notebook we have to navigate to the correct folder\n",
    "os.chdir(gdrive_path)\n",
    "# Check manually if all files are present\n",
    "print(sorted(os.listdir()))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up PyTorch environment in colab\n",
    "- (OPTIONAL) Enable GPU via Runtime --> Change runtime type --> GPU\n",
    "- Uncomment the following cell if you are using the notebook in google colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install correct libraries in google colab\n",
    "# !python -m pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# !python -m pip install tensorboard==2.8.0\n",
    "# !python -m pip install pytorch-lightning==1.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we first import some packages to setup the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import platform\n",
    "print('Using python: ', platform.python_version())\n",
    "print('Using torch version: ', torch.__version__)\n",
    "print('Using device: ', device)\n",
    "# Machine: 2015 13\" Macbook Pro, i5 dual core"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAAlCAYAAACAjKdZAAAUzUlEQVR4Ae2dW+hFT1XHV9lFsovipazACo0CTbEiqEiDeuj+EobiQ4WVaNBDqNBDF/CpFCVQ0BQ1utAFutdDQRkWiOIVCXowNSMqKitT1C7y+bm/P77/+c/sPWefOXPmnDMDh9l79lzWfGetNWvWzN4nYoaJwPkR+LSI+LbzkzEpmAhMBCYCE4GJQBGBT59zVRGb+WAgBD4vIv5/+WFgzTARmAhMBCYCE4EREfgcm68+u0RgOpF9RkR8VkQ8LCIeERGPiYj3RMS/lyqY6VeHABb5Z0YEDMTvsRHxjxHxDyfoKfX/91Lv50fEfyVtiD8xvHKB57lnpXTq2HpGnrTOtTI5uk6VNuXzVMj2qRfZeqjpV3j+YxHx7j7NX2QrW7K39VydJh9hVNleyJvRBgIjyNCXRMQHFzqZK/9njebvMUsM5tPvBWuF5rOrQgCm1bh7/LMn6qXa+LJM/Y8u0KIyilPalL4W5xYKW+3lymTIPlnSlM+TQdulYl/pOm+em6+6dH5nIyzuHavS9RaGTISlsqRvld9J/izWGIGRZOhJC09leech1nEy/EtEvCsivtHSfyjjSbDH8/KKEEDJwAN4p77W+vXHEfHXdt/i8lVLGz8XEb+bqfB/I+I/l3afnnn+6oj4jYh4Q8Kf71/KPCEiHp6U+8uI+PmI+M3FE+uPaY8fyvyL/EFEvDAifjEi3puk97yd8tkT7fZtwVt4Z/80Ipw3f/oEstWe+vPUiCfgwxnMRM0fRsRrFpn+gBIz8ccX2X7K4jFUlg9FBPijE9bKK/+Mz4vASDL0zxHxxRHxTcsuyF/UQIMSl4WPF2OGegTwoHxdffZhc9IH8UBrr6WvIOWiXwPieUYLNLFy2Qp+dosyP7hVwJ6/Y2nvjZY20uWUz5FG43BaWFRItnLe2sNrvP4SqffqD3Z02XVC6uneUd0sckYERpAh3+Fhx2M1OPMxwcxQj4D2+1sbIvUUtMvpxk/r/sgwqDV25HbVZFRjjGksVOYZB0BDGWgcMUz5bD8qLCK+sn21xRrFk8SXvHDtiVsqzyy2Dg3oGzCfRtWhyG3n78kLUDOKDGnR/3dbEPHau4jew7xb9V/zcw4Xg11rQ+QcmJ3KsPpm46/aScVpAd+aVX6qiGvHRILCCnnEMOWz7aho1dnLkPZzIpvKuG1Xm9bWG7dUng/1Jgv3S8a86QA2rKw3L2gsmQvOPZ6a86EFB8BdyE1sP6yHEcHZmhnqEfjq+qw3m1MufM5I/d8JUWDv24PePvS09JrXZ18RES8d2GM15TMdtePuUYgEzub1CH5+9ZU9GjxRG71xo713Wl++wK5rLnVG9GtqMs88ByHQmxdGkiHOATKXEX5/ibMRIOmXM7yyhWbiHQKsosCu1jsyMmzuJWrVHz8nsbknbeCkb+xB21pIV7eMCXvyW+HXlvEbme8lm8Qj07mF9SjPxSu9jj2Ixxi/ntuPrfHujRv0syhz/oeGmiAvbys9VtPmLeXpzQujyZDPldnzv35+49wutktjzFMe9j4HFs4srRSSttlQjocECa6UKp8eWAvQq7yKt/hZ/T3kLNYaDad4NuWzParirV6GlfiRmG2ESw29cQMn1x/gV7OwII8wv1SsR6e7Ny9oPEeSIdF0N3+kgv0NNoJyU8OYj48IPmbH6+98LDL9kKMVu8lL9lbfYj3/QruuvcQr810R8cTlFWNe5/yt5fMHa3XA1Hw2wz/s+rdWgDH+iobjd0x9P7rQJdepkXnQ5Vctq9dcIVYMvD79/Ij4joj47iXTf+QyW9qblms+xTBqmPI56sjU0YVhrPC+5cOCbD/Lc8VHB3ud9RIdlxT/fUIsHy/e+nDxryxlvjwpO9ot+v9xywdj4Y10joVHNAfz2ZfVj1KO1rmG9IwqQ3zK51uWeedBc0jqYktXCLLKWP0zod96wMUMFsJlLS69iYIhoNf7Kc8Y+D31Z92Ly4ot16betvNXUj0fyrtUp8ZUHhzKwQeElxT6WlOfVjXUt+VxWpq7j3zVSfk1D5q2CygM5ur3msdKB+o1wd03PNjFlM/2AyK+7OGx0pYUPMlY5uQJWSr+VUb77u+usSduItJ1EhhuvcSCPAtr1TFazFjn5hCdRUX3wRPSY4pH8qz35IVRZchtpQfxmAbNYwrIs+VnZA59K+NBjV1BAtYzypGJ3o0hsCGNiZ0fedgqTAO4Cuv7NwqWTOm5opwSgaFpR8aE6nJ6YEQF364k71pwJUYf1L+99Xl/cn1Zo4VnrnygJxeol36JRmd20lFSaZBSuAR+1vh6POUzHdHD7jX+PQwrN4w1huJV6NAZTZ5xP3LoiZtw8K1wMIL314Iwzsn9Wrlez1jcikbpRAwtpfkiG/3p8wV5RulXT14YVYZkxDMujNV9ENO6dexuN2X0jvUeWNqj/VP9ag44C4c01jdSAHZL4FXWJ376lAY3Rqh3LcjrQj79cuOH0aXnqTHn9bthpfzH1McKS/XIUPf2tq5l2FFHyfsE7/JTcHwpl+NX4ZHrm+oZIYY++jCyfI6A06E09JwUxP+KU57ziRa+HDn0xE04qE3ht4aR5Bq9OGLwvnDtIdVb6GKC+qT+5/TZkrVrpL70WJyo74pHkSGXXRnJd4OQ/heZBjMdIXdf9x7Y1NAQuC3jvX1yYVgTeMfTjVT6kGvbDYrSmFAnRpLjkPOQkY8VsvKt0ZkaVlpZO/2H1Of45PqZ1pveOw45AVa/HCM35uhzqsC0OoSnRw+XIJ+jY5ijr9ekIMNYspf7ThpyoefEI4deuKUYOD45PUB+7ayUnqd1nuNeC3HXV6LDdaV70j19JP7oxQsjy5AwYFye4ROcDhYzuPx/W+lQYGmCFVOcMuZ/7B62HOLjIF/rH3Wf8ttKKTZvTxJywuIHvfkn7VL4N3vAf2C91e798m/8pvKa+v6skHdPfYWqVpPfvPIUpuY/2PjvMOfbv0rKPCq5/+3lnv8CPFXAwMXohcZjwiXI5zH9oyyrvj3ezGPb7VHeXzxAv7pnVe27PiYtvcejjrEgr6XO4aj8LcQ137J62wKEf++oNTbHyvXrIoIDz66vROMP6CIiftKumQv4715+a3OBFbmqyxYy1AOQx7kS09tTNPzildafbM9qjRBWEEzOOcPBqqu6/EhVrsvIhKeED1fy1t7LGuFDz8G6ZWhZH2+81PKN98EVKjzIpKN6fmbJ+P1eIHON4YxxTmClCM9/r9WzPGoWMQHqj6DBMOelqG3sUuSztj+ej7Hkz3AZR4yO0hk6L7N1jZHmfzJfyv/I5QEfnKwtw5/BHqqH/MOuuW1/yBA/l2jVtod4iom5dajFoBduaf9YYGkOYsvF9QB58eySzlvBh45R2lbp/li5lsH8I4UGeLtM4d26WN4EfL3db10eO++OxgstZGgLs6bP5TrF8CmdX6FB34qrdbNqu2tt26lpZ85Umbtp9/YVRsYjyMrUD2szLvwesHeb9NO37ta2tjzfGp2er0V9wmeNv5IuPeBW5YWFlBOYkcbzNICX8qf4adWflml1D33eNtd7vVbXKJ948TCgci9eHDsGrqfSMWh1L/6rpdXb9QWtl3eZI3+uDeeFNX3g9dZej4hbSnuqBxxLrsEN2T5VaCnXORp1POHYfhw7747IC61kKIf7sWkP2AoUU/oBP32/KtcQqwCFl+tiJaaxZy7Pa/5SZKWqq37EKuuXzbtBZ1m5Ixy/tHwf4xAA+M5Jy9Cyvq3vSZXoLq0+/2QpkOPbDySVsRWIxwzjlVX/Kb9tgxJoFa5RPvn7p5+ICL4fhtdw9e8gDgTyXyPi1w8oIx1VW4ZvDG15l7x5JikFfb9K9x4/1W8KbXyf5Un52x7tuhwNt1wn3pUkfuki0yT/zvKMbwGeKrSU6xyNfMtK4bW6ODBuMe+OxgstZehAOA/Ofn/kxFeNa6sgtxjxFGwFX4Gt1btVzyU895VUzhOU81aQlnqm0jf1dMgR7NcwdKxz7QvDc+UTPntXk043WMB/SnPDQ/0kBl/nWeFCmh8K9TItrzHg6C+/Eo017d2CfOIBZ1w0RjW4tMpDu7Ue+D1tIrfiw7VtTmGwRo8+yXBKemv7uEZnbR2H5pPMC099e04v7xzzZnctLa3kOtce/K++lV5AypXzNMdobc7wMsden5oXWsrQsX3Nlfe55gXyWPn5jdIqiMFSYNVV8iAoD/G3202LPznFDft7VmfrS1aiz2pd6WIA8FVlznz4SpcD535e4mmZtmsM2EyxYZPoL+PoOOwh9jER8efLalVfTU/rSY1ZeFAfT+Wr7KcOHPhn6+bYcCnyeWw/z1E+5ZFT0PCdVmnpjAwyobNDZM/tCECrzt/8qtV5jsseuOX6lS7M8OD9QkTIk/XcXKHGaa3kOkeWH2nw81WeF+wxZEqh9bxbakfpPXihlQyJ5pPGGFau+DkgXJrwOFyq8CJdLCtM/kJE4GoF8fHlMKqyPmG5YEsw9yaE8q3FHJj0SWYt795nz17BYG+duXJ4MGRU8fxbc5kKaRxi5dzTjxWej5jMwXGFNaWgPGmcvgWDUcUqBgOrFOBlDo0L559aJi+2WWsWBqV6e6Zfknz2xOVS2/pEgfD0DbY3ZPLdbzFEBAYavPGciOAvtNiuZ9G5V7dmmhsy6aMJVcwnMka+fsPgSIoOd8t8LA8TeutjGQrxYvH3aeki/ZTzboaMsyYdI0MQDsYssD932bn4o2XOJ/3HI+KflkV76c36XOdl//Dszvnj38cpueFlIctFKU8XlZAm92vuYJ/KKE5XHDki19I43IcX5xQ/79caDbln2uqinymOcs2CjwKYCZO17QGeKZ+EjvpTHNVGrn21SXyufMUv0zpxK9d+qFN4iO9Wit1/MV5liF0I1sqO8OzS5HMvZtoGS2Vnb3215aTbTrm15rqhxHt+JKD0dyV+LEB4EbuOkJFR2/+9+XrgVqIN3efyzHWPrf0SPYemo8vgc34+57DFqH6V5gT4JH0BqMe8u9bHHrzQSoZ0qB+50bY6mEuGPI28tcHn1bt5uub8hr8h4I1RAUTJWoYIKQ6fSPVcz2qJvaR8eKAkFKlyExZuWEkxUibN7/12JSLDivxrhtVafc4APfNhCAuf9ByZ97d0LeFVHcSulErlHGfKoLwuKdyKfGqcrtGwkvyXeNaN53TSdF5F6bs+kF4lD54MyUYPPSt5ZNx6B/GK+kuMsXIpwel2fvd+5XSz5lv4JQ0ac+c18YeepWVa3ffgBe9XTu/XyJB0qc8/SmNMhLnGZ00WU+y8/Tv6VAmxT/xeUANKHl9NIeTpBK9yvroq1au81xD7V2HTAdHgeT8RKGFfgyF5tYohTsvo8Cb51lZvng+6SsHztaiPdtRfVyal9tP0dFWms1JpvvTelVWKWZp3xHthRlySo2uQT43THt44Ztx6TArOu6lh74vWtTEWneIHFkgefGFX4hPPf+y16GHcegd5F4RFrS7oTWeuPbZvRTex+N1lmPS0T85DuXqVdo55twcveP/3yJDmZ7dfwExzM5graHzciaRnpVg8eT/3q5L7hExJLDDlY2UEkHKZ+RkQLypFeQ7Bczp6XgsTsJIRpL9VSQXFMSU/OOmgOqsvbRViCWMQCH+saq45Y8U4EMu9qTyqjzr0NlopHzSTD3pa15diLzr38oT3L627dC+8KJtORqUyo6T7pHsu+cTAhof2/lCCdyu4DVClLzTRbGRv9rjHpACx0gPwoXSmL14Y3zWDyHkh1SXUL8Xea/HQC7fcQLtM9+pvjo49aW4gwAvcyxuDLvZx1HzgfLIlS5KjvTp2T5968cIxMqSyaf80t/qxEjCWjKb5S/fIL+Opef9+wqbhteCuLirgJ1djWk5Ak6e3okxp6X0v40EYEZcsX4QKg8fz+rWMIleqPJenyYUUBmFw+XEthkEoCao3zad0xrJ1fUvT95FvV2wpiPtCdiGloT7Zo+Il/OeYFTMO+MBXt+eQT5dj8cmeuKQnHHKNbW99oT72mIjcm+A45gwlx4ZrL4ucpkGKXbohfd76viduKe1uaKAbLy1oknceYPzAlOBeFOWBP3PjvhS5izQmlOkpR2p3dBkSvsLMj6cwN+0Nfv4Xz+NdYBV/f6PEQkwFKElNwoVsDzggXVt3qa5LTMdooN/gVCv4uCq3yoD/HoNkFAzdcEvduTU0shrAED0kIDAI/KXidm75xLiDh/f+oD9VaLnxY4x6TwjQ0XNSoD34EEwO0Q2Ukzc8N3lRpybgLQM8h/2etN64OY1MiOBQY5B6uZGu0YXiA+I00EfNB8wNNYF6xAc9593evLBXhlIMcVwIL+bWvcEXPXvrqCrnrlq3slEmPQe8itiZqSsC8ujlJoiuhNxwYyPK57kMKxn7eAxGDZq4Soan7yZoEibtmMliC4tLwG2rD9f2/Fxyfam8oOM2yFUaDpEfGWcn9xJKSfrkKfDv9yDTnsz7m0DA3a+H7mPfBEAdOjmifIomJofege1xPEKjBryFUt65hameqQ/aQs7lbdnH0XFr2ddLqEsydI55d3ReQBZY1PtOCTQjO+n2uTx/OU9iyge+LY2Nc9KgAXYlKU8FE+sMt42ADmhqIrhtNPr3fkT51BmhQ7d6+6PXv0XfakiVtxasTBDsCBCka5fbGd0IAiPK9SjQa/GBMUVwgyjdUpYuWrKuRsqb1rFaaO9DNSYlqQPabi3urXuWuw4ExOiaDK6jV5fRixHkk20q9AHnHKQfxBO41Emf+uJT/KTzVYxbGtywYoUtI0wvvqT55/31IjCCXI+Irm+lI0voHukaYtKQI7bRhWGNt0ovY8lYO3nf5Yp24o85dX9ygmcD3RHANSv+gPFn6IfACPLp4w8foND4yT0v3uiHyrgtSdmXDqZjiAov4lK+cXs4KWuBwAhy3aIfp6hDCw7JCTLFIXg/Z8Uz9A84bgU31mpfUNuqs/o52346TFldaGa8GQTE7H4m4GY6P0BHp3wOMAgVJKDEt/QokwR5iGe4bQSmXOfHHznKYYO3inR+tUGfxOCQ+wwTgeEQ0Hmr9ADhcIROgiYCE4GJwETg5hGQUeVnyG8elAnAeAhwlgY37NwSHG9sJkUTgYnARGAi8CkEtN26eobxk3V9Hwvp+6GBAAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple Recurrent Neural Network\n",
    "\n",
    "The recurrent loops in an RNN allow relevant information to persist over time. A simple RNN architecture is shown here:\n",
    "<img src=http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png width=\"150\">\n",
    "\n",
    "A simple RNN takes not only an input `X` at time step `t` but also passes a hidden state that is the output of the previous time step into the network. The output of an RNN cell at time step `t` reads in Eq. 1:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "\n",
    "<p>In this task, you will to learn how to implement a simple one-layer RNN as a class in Pytorch by checking the code we provide to you.</p>\n",
    "    \n",
    "<p>Check <code>exercise_code/rnn/rnn_nn.py</code> where we defined a naive <code>RNN</code> cell you see in the image below. The vanilla RNN transforms inputs and hidden states with a single linear layer followed by a <code>tanh</code> or <code>relu</code> activation.\n",
    "</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "<img src=http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, Pytorch already has implemented a simple RNN in their library and you can call the RNN with <code>nn.RNN</code>. We will use the Pytorch RNN function to check if our implementation is correct and compare the output of both functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import timeit\n",
    "\n",
    "from exercise_code.rnn.rnn_nn import RNN\n",
    "from exercise_code.rnn.tests import rnn_output_test\n",
    "\n",
    "# choose your network parameters\n",
    "input_size=3\n",
    "hidden_dim=3\n",
    "seq_len= 10 \n",
    "\n",
    "# define the two models\n",
    "pytorch_rnn = nn.RNN(input_size, hidden_dim)\n",
    "i2dl_rnn = RNN(input_size, hidden_dim)\n",
    "x = torch.randn((seq_len, 1, input_size))\n",
    "\n",
    "rnn_output_test(i2dl_rnn, pytorch_rnn, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, our implementation is thus correct and working similarly to the Pytorch implementation. You probably already expected this. But let us show you something that might be more interesting when we compare our implementation to the Pytorch implementation. Therefore let us check out the running time of both classes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "runs=10**4\n",
    "\n",
    "\n",
    "print(\"Time Pytorch RNN {} runs: {:.3f}s\".format(runs, timeit.timeit(\"pytorch_rnn(x)\", \n",
    "                                       setup=\"from __main__ import pytorch_rnn, x\", \n",
    "                                       number=runs))\n",
    "     )\n",
    "\n",
    "print(\"Time I2DL RNN {} run: {:.3f}s\".format(runs, timeit.timeit(\"i2dl_rnn(x)\", \n",
    "                                       setup=\"from __main__ import i2dl_rnn, x\", \n",
    "                                       number=runs))\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! The Pytorch module is faster and optimized in performance. Let us agree from now on to use the Pytorch module for an RNN since this will lead to better performances. However, it is always a good exercise to build the functions by yourself and we really advise you to check out the implementation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Vanishing Gradient\n",
    "\n",
    "As discussed in the lecture, the simple RNN suffers from vanishing gradients in the back-propagation. The hidden state is manipulated in every time step along the sequence and the effect of the past inputs to the final output vanishes with the distance in time. In the next cell, we will explore the vanishing effect of previous inputs in the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size=1\n",
    "input_size= 1\n",
    "\n",
    "time_steps=50\n",
    "rnn = RNN(input_size, hidden_size, 'tanh')\n",
    "\n",
    "for p in rnn.parameters():\n",
    "    p.data.fill_(0.1)\n",
    "\n",
    "x = torch.randn(time_steps, 1, input_size)\n",
    "x.requires_grad=True\n",
    "_, h = rnn(x)\n",
    "h.requires_grad\n",
    "h.sum().backward()\n",
    "grad_tanh=x.grad.view(-1)\n",
    "\n",
    "plt.semilogy(np.flip(abs(grad_tanh.detach().cpu().numpy())), label=\"Tanh\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time step t\")\n",
    "plt.ylabel(\"d h_T/d x_t\")\n",
    "plt.title(\"Log plot of gradient of output wrt. input\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Note</h3>\n",
    "    <p>It can be seen that the gradient of the of the output at time <code>t</code> with respect to to a previous input decreases exponentially. Hence, the final output does not change significantly for changes in the previous input and hence the RNN does not have memory.</p> \n",
    "<h3>Task</h3> \n",
    "<p>In order to better understand the vanishing gradient problem, calculate the gradients <code>dh_t/dV</code>, <code>dh_t /dW</code>, and <code>dh_t/dX_0</code> analytically for <code>t=3</code> and <code>h_0=0</code> using Eq. 1. This exercise might seem a little bit tedious but it is really useful. Can you explain the vanishing gradient mathematically based on your findings?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Long-Short Term Memory Network (LSTM)\n",
    "The vanishing gradient problem had been known for some time until Schmidhuber (1997) developed the Long-Short Term Memory Network and showed that this architecture can overcome the problem. <br>\n",
    "An LSTM is a more advanced recurrent network architecture that can learn long time dependencies. The architecture of an LSTM is composed of a forget, input and output gate and the cell can remember values over arbitrary time intervals. The standard LSTM cell is shown in the figure below:\n",
    "\n",
    "\n",
    "<img src=http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://miro.medium.com/max/1400/1*-kBdBYzR7lpimgb3AIRkOw.png width=\"400\">\n",
    "\n",
    "Compared to a simple RNN the LSTM cell has a hidden vector and an additional cell state vector. __What size does the cell state have?__ <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operations inside the LSTM are given as \n",
    "\n",
    "\n",
    "<img src=https://wikimedia.org/api/rest_v1/media/math/render/svg/2db2cba6a0d878e13932fa27ce6f3fb71ad99cf1  width=\"400\" align=\"left\">"
   ]
  },
  {
   "attachments": {
    "CodeCogsEqn%283%29.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVwAAAFxBAMAAADNCg33AAAAMFBMVEX///8AAABmZmYQEBB2dnbc3NwiIiJUVFSqqqqIiIi6urqYmJjMzMzu7u4yMjJERET+R06iAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAeIklEQVR4Ae1dfXAcZ3l/dB+S7kuntRzb+dQJG0MaIFJMkqYM9slW6Lgt4QweKJhpTzAxtCWylAYYOk048Udm+BypSQiGpDknIZlpO+EETRuGlp4SCkMzHU6kZMK0Q05x4zKFuBKGkmna0t/zfuy+e7d7Pju3pxPdZ+Z2330/nve3z727e/f+9nleIkdum3HSmyD15GWbAKQBcXNZl0K4xnfX8WRo3Y6b1FC4Cay7c8/r49N7irRzYpZm4gdvKwF+6pb9ZcqezN3/qppMG6e00cnpAg1eSBS7iGjmMCV3AM/NFL+O0ouvrkxWZHqjMRr9L5WJriJK5wE3R3SaaKBONF+l6C30nYJKU8/ISo5oD1GiRjRdJZojWi4RRdcoegkwqjRSPSJD4zQ4WaYI4EzjM1ajdSDPjlN0HIcq3SNYASN1EUWis3QEyffiA7gT2EUupWgRe5VGqkcks51ikWG6F3CO4jNWy4wchyzweCCd7hGsDGOVPpG+iHJIabicK4Yv4Mp0D22Xyjm6Nl4BIgmXLAmOravTPQR3ZTZPY3xj0HD58oIIuCotMnpjk724SssfYyzKussLSOckXJXuDaQCRWobLPkHnFRwY1cTxeWlRiqdKomqvbDJbMEj+FJGgqcFrReIDhToG0R9Rc6T6WV+ZPSI/DeMuQYsJ629tNvaW6Dkz/dXaGhi5N+RKdL09I96BGubMBJt1uuRavzQ20TyqU2EFVDzmwpuurKp4MY2FdoQbGiB0AKhBUILhBZoskDynqasHslInrms1Agl89KzTXmNdTboeHHHPE89uiSbW6m7MnrmIGnVV7c3opkvPJVvzDOOc0baTPJkW8AyZOWsixv72NmY4T7mf3dekvXK7GzeilX5u4VGlZjpbSEZP7jPtmjUoaITVrVZ0380Zxk5CT+49xmVAkouqmkxl/rWcPv94B5zKQnkYA4TOU3SGu66D9zsaJOiDmesWixVofVpjSJ51Dr6fqLHpy7HVLokgZJT1yen1iQX9PXpbUdvVDhUfuLOWzC5fco6erRA9NdTv9JhlLa6j99pjR4/XhPHy2L2SST5Uout0eDzmgT6KD39YuYmxQWJ+UmpQuW/Vkyo0T7OHMjR2xZkaee3act+RqRKtnqG+yZ8voIZPyaBEpdSEmzWQF3yQvprIJWfqBKdIpJwbyMaDGxaLWVdgH7oGd44ArhxvhlHMXfKXffVifDGg+aFbLgqP4t5y8mqhJvm0xdmxr7jErGGWeeq++sD3AEeGtgIEujQrOCGFBfkDAaVn8UpzZclyiy3O4NPIJK16tCbsUou7YDbx6eRuJiiRewZFrghxQW54SI/UwDckoQb5XbgkIKRqJWD4kHLrR9wV8aRn9gqJ9KjRaJRmwty4Mp8VLz/4FxJwl25jskjZAUieAZDb6SB5WmEm7qEUhfYXJADV+Zj4vp627riNAOBykrFM3ge916XefVgiGEwrKFW/HT1obLNBSFLzUSpfDF2S8K6eTGI0CQYkc/glYbfZICb5TsGNgJuJPPxT+OQLzVItC54WaRU/lKex26S9oKGEZca1wpE5DP4BGMzBHDTeHOAVmYl3CFZprig/qK2rsqfq+JGVvo+8zF50S6wH77T4qf55DjwGI8JvufvhinvqCnr3irwKi5oYFjQyMiKyPyxMuj6hSOg7EEpgoChd4rqAWwsNiKtzmJjPIT34DDyAg3ip0FfEen4hGU9V9JcUPpiejMyISr/iTol3j77KC1XYjV+HKfzorTzm7R8qInbg/0Th3mgCtHDr/gpKRIo8ZGvfXU3fm9JLoge2VWVUFR+/MqD98Z/UaLkc7+F/C/dckXngUqNKauIRNIqRPAV+stPuGgep9AgfvkN1Tp0+N0PRqw8dCVG6KFWKtP8YKXIWmMdv/zGeh06Xt8+JO63qcvSGKX+MjjMZfhF2SB++Q3VOnVoXbgirjQ68PNaS51v4NKnSk11/PKbKnYkY/3H67m2FP0Ad4Ik/mA0il9+Y73OHEcmftimovunpq7w+gL88ttUG1YLLRBaILRAaIHQAv9/LTC41+vR2rP2kNMMbngPVNzHPXR0SM3tmpDG6uZRQzrXcKwPA/vvqzsQ+8HXuA7FwfcLzXl2zpqdciey7sMeOdpI0uc8TLCRpM95wN1I0uc84G4c6QOw8TPX6ssqc0yDj52q08DJtcN3449R9uTaw696EbPoyEucupQ2kPQBvCf+ePqNCmXanjGNz9cpvfSzPJ0oYf9imZZz2Bcp/gBmG8R8pGzSbdKHMtfShD1T+pjCTdRfx+c5zDdhHwXfk9yK46JkKxy4XSd98Bp32gIcypSwcYRh9oMcwUQjRYuECb+CgAtvFcO6fXVBBnWP9AENFbHGAWdIzo0gJUTABVQBr46s+ZwH3K6TPhSnPjFFtsJUmCMCLk7Dhnuo7gO3q6QPAC5ZBWzn8S0bIuAWDbigG3jsugdDFDmCsu4W6UOg9kSHYzwiHGkPbtdJH/APIIQLg6B9XOZtgrtcNK0bkyfWddIH1741nrwElGXFMS1STXDn5aXGnE60vmGkD3gHK58t8ty5P9wiinAjG8I+CrgYw8q6XSd9wENZ1fkyxUYYrfOYYGj2pYX7bxKPksgw0WcAdwNJH/wksP4ct9wh5qqch7Dg2JlnZyIw+qMqnVhA8Q5K33lhjTaO9GGTvonfJ+zjB7HzE2doYlsOTj/1yPTI8xirH5sSfzi+tetzsWuep40jfRiuEFCWr9Tpxj0uLR/pLuljgFgcTxaNQ1fSF26XSR8D1FL9m+5bg1HmC7fLpI8BKbbvVuPInfSFS90lfdyofI/6in5F3SV9/FC483GXqLtznKOQ9HFsEaZCC4QWCC0QWiC0QGiBzWgBb1on53cqvgV+DTqc703rrPn14lvg16DD+c966fMlgXwLvLQEkXefl1JfEsi3wEtLEHk2b2Eq9yWBfAvM1gGmvX15/Egg8i14+RCTuyeOVZQaZ2aE0rv2LxBx6M0/na5JX55WJBA9OFV48C1Ex389T5odEmHjXj5Al4bk9Ov//vaRqswz5p3Q9WcK9CScrd41UZMOBq1IoIG1zLHS07VITbwaLekWETbO1dnLP5jcVsN86RuVIntWL1bEq93jMrbPnILbggSixRqdKBPdg/k2tBRwB+qYla9SRyUmkK6CiHrGpfdQBYdX2aGIpAtPtIhMTxKI3YGYCDpToBhmLAXc5ZJKoFGn5IS1AFWr6M3t88MWJWyOYsf+MPu4QzFDMi9npt00RRxTmP3D8FOo0iCILwF3Hc2y49yuc7JqQSlNjDT6/Ah/xsWKB1xPEkjABbTvwRsHJJKAq9yDOocVlLAFu4qt2+dHBk5bzHvA9SSB6BoMhjWoSu66W8ENIlRcysKoJd66fX7OGe5SlZYKeNv79QVt3SBCxcUExZq1Lmj0+RGDYdI1GPJy7HqSQPQ7P07+Pk6cx660biyIUHEpQVAdsnLU4PPDkb443BdfanzNsC+P61Jzk0CUe+ZzqJXCdwW4ZVyTEe0eBAUdE+njwV4/DT4/7NuDKIbCf3ICQAAb1i2iY08SiOooAc2Cm1hsR2ahn9khVoFz7KjMgZWIWS/Av2ic9T6mlUdugi8FHh5jMNcq4ApfHna69CaB6FWoA3YIN7GBC+MVwQ4p9yCtsSP7rHXXEX5ISJ8f4yH8uhrdXhCuoN+evBd79uXxJ4HobZZ12aspjTFz1/tSBckOHRBh4zoC01by8MltL7FlBG9p/MRJ3nxlHtnpmw+WDswgBBz78viTQHT3F772J9Nr9NafHq/97ouKHVLuQXZXHUyc1eeH+/Kf98/mUDzIdFd35Gw+PwKFP9xlMXSv7A5W9HI2n5+zwJ0X5fjR2S05m88P4/C3brSC4syvdgtse/34k0CZf8VoeKjUnpou1WpFAiV3Te3/2y7hCLsJLRBaILRAaIHQAr8cFrjh6B7PE+F3zU2JnRoXh3pvlnUxfYOF/5Uekl6qy9wH+McWpk4eGHbtxcEGbCa2eHfaX5f5Y2qflXBJ771b5YzsiJHuVDJt+fxl0XC1/w/+3QrRe28Aa0Z21kh3KpmQ0SSa1Wm4ukTD1Hud79q7KB9PtshV/dwPsuJVeY925wXXRfl4skUePZ1LVtSa9a5+XnBdlM8xb8UvK3e54cVoW9l5wTUpH2+2yNZ/folF691/OIKJDCnGvEh/Xfj8sP8P0Tt2fUGMWb0X4dxu26dWBuK26YPH78oph6AAI7+tW5O/8S3hOyE6FZPpnKL+54XPD/v/UOoDFP8w7gx6PyDCuT05qlYG4up/SYS4L2LanAKM/GZZN2L2XN2kjDk92+eHB8V8ARG+UEfvb8OUDab3ZnJiZSBGS+OY7VNwE9XAIr+lLRBVcQt9N/r9aJ8fvOUPRyawIsP2XodzmwauORRB4pdio+AG6ASU4OnzQXHvbfD76Yc1md4B3NgOYAFcvdfh3KaRPVbDBuc6in1FDoYAI7+J2650U2rw++nH16vgDnF4KsDV+yjOhHHC4UfDpcXRR3Ekxm6mgGFTkuSWroqyTkgf33blvbfB7wcz4Bouu0swXL3X4dwwpW7DHZy2MOckL7XgIr+J2+6yVeaOYU5DWsFVNU24FP/yRE7BDS7y2yLfucbAAjb5/Rhw+9Vg0Pu+YXlaBtx4WTATsG6MAoz8ts6/dkGr7Wzy+zHg8upr4lJTex3OzYCbzqHKdTxHGaGlPI/dQCK/Wfi1ixtDZmuT348Bl1erYcpX73U4NxMuj4+XpEPQXDWoyG8iIChCfA2MN/n92HCLxPQ/ncAI0PsDBRHOzYTLv5pvlA5BY2XcjgOJ/DbIz7PMxI074TEFFq3R70f4/IyDynqBkm/ZUXP2Mpwb/8sTfCEabilTIi8dgp6AR3Ewkd8yJ0vo8h17Pujh9yN9ftj/B2HfLv989hqwb3ovwrnZKwNBRbryZRHElqPAdSXyWyu/H+DpNWnl99NrWIGnld9PD8Jt5ffTg3BDSKEFQguEFggtEFogtEBogZdnAX4BnT+bRH4bL83yZ7MIW9ZtXRepk+ux8zgNPPxxxEXqrDn5PZFqhmuSOi62pxfwNsM1SR0X29ObcE1Sp7/nB4OL1DHZnuCNC5+f9xRkN+BCtKQ//IoK0nLBHnMwfOP48bfq5XyCdvDRYIx9enXLF79ytcwwVhj4bJUexxSoYHjEXUHdGVJ5EeFlHzeIBO3gY8DUyaWRMj3Ar0dDUiWxwyYBbmWwQorhMawbReGXlJPKPQE7+Ggwxn6QqRSLZ/NcgkUxaH5NMzwG3EM1zPQquGcKwTr4uBDJg0PMTTDkZ1yFkyVEBVzQDI8Bd+V/8CUouOvVYB18XIjkwZxVJXqw3OihJKdBNW1jwE1YLxXQUozd72HAgMIS9EkADj4SoGub0a+2NKxKNIHvHPGp1II9Bly63RopKbgBO/i4gMqDFE/2s7g9lEjBHZeFJlw6shNNhHUDdvCRfbu2Mb36l9tDiV1PIH3DsrIB913Imasy3HzQDj6yb9dWv9rS6KHElxpuAUz/QQy4uBlTdEG4AAXt4CO6dm+SMobe4UYPpZU66uU0w2PAfQL5WeFolQ/cwccNVRytM2OTGpUeSs5jIoVbW3xBL9hzCnX4A1mugLIqCBegwB18ZI+u7Q+se979sYkF6aFkPITvKNNhMD4Ow6Pe5VvGon93Sheg4B18XEjlwcMT1ns+rTyUzJ84v9gFO8oFe05aM1j3Z0bUfzK1a38Zb1qwC1C3HHwkUNfW7wUiV6WeOWjLQ6ln0J51VaLeQSqQtOWh1EOY2/FQ6iG4IZTQAqEFQguEFggtEFogtMD5WUD5+pxf4+63Gqu36DPXomxjirSvj2fva565vZrZc2RPa0P1HNnTGm7PkT2t4XaX7DGx/NP0+yqp6d9TWY7PT7tr/Zi6gk8n935qcXTs0bGc7MqJNNPuWj/BQzR7WM7RkHVRkl+SFvKY2mPiro7Pc8IrpVWYN7t+VxKY+eq3ilmr2NQbnGfaWOunqVmgGVh4hBf0SI1U8Qp6ydWVgAuji7BudRTN55wlMzbqMVHAK/tWGWDwrXssTTRuwD1U33i4gDkpYo+B5wHpZIiwbtGAC6ZCOycIbsqo283k6qjsrcFFqUfhZiwMggwQN7goNcFdLprWjXXTorqvzGp+kF0Bn1poclFqgjsvLzWmg6J1uPZsgGBVogG+504WmlyUXHCLwAaubQh791o/3cWctfKHAJdDc8mliZzHBEOzLy18AT5h3rqKNzX6rpnJLf8wVwHLM4KenYdwm2v9dBUtosCO5JMfGnklehUuSs5PnDbX+ukyXKc7sTSRc+hK4dLqNWnlotSDcFu5KPUg3FYuSj0It5WLUl+x14ZuKzytwry1aheWhRYILRBaILRAaIHQAr/kFki+oWdP8IGb3NByOIyc1SFpQ/63M9DGaG9ryMvcxSWtBO+fb4gkRYAkp+s2SZ1nnRZdTUXsmWjZbZukzn1dBel01m/xt+9Im6TOMadFV1MrYvURp8v2SB2X04/TOOjUU1hj3Bo1Vj6W8dpuAAcgY7ll7nzdAkAYS/aYTj/CJeh/9yVv/mgpaKRCf+L49Mjxz7vmmnhssGeljOX23VrmKmQ4S/akDKcf6RL05OgnEGemK3Cx7Px27siZyZMT4/xu8UwOL8nnRUysgTqoiSqxmE4/yiVoogzGIM+FgUum8bYr4Z5GxyKWG0bCUpWWS8ApL0nD6Ue7BF2DyiJUWeBoVaA3ox8DLnLHyjBrWawjkh0XtQynH+0StBcFg90ZDTLQmwAiNw7c9yIDId0YruHRYzj9aJcghhvHaO+CNAXZdOAeRfcSrmvJHsfpR7sEMdwg1p3xOP1Dje/Ie8M1W9pOPytyeLCLSrfgLlk1XqnJEcCNSbcZx7qm6QynHxBtQhhuepujIsDUHLrh24AtUUHqcJYBdx1DWMnj2CunH+0SxHC7dKlNbxEefRqMYE+arWss2WM4/WiXoC7eyFYvpOWc+ZgYGKYj0sPHtm6JnWnYBYhluYJTkk4/2iVooorHBLK7IPPbv73VRfjIJXrYw0fHclsEkgP2kj2G0492Cdr7N5R009+BAU+eGS3j34Pxe5DjtZ203oPPXtpt7S38lXVNiZwle0ynH+kSRHsHd+2vBoaw44rFjazjWgNTGMINzLRQvMms250HWqcMrh0FO6Uv1BNaILRAaIHQAqEFQgucgwU2ePUfB+lZSCDlGaRXAXLaGSkf76GzaDYUnEPyLCTQWF3qQkR/LxEEka5jVMghfRbNRu1zSJ6FBNKeQfgD7SWCINJ1jAprSJ9Fs1G780kfuD4EUZv0Uudhao0+cH0IojbpJa2883sfuMaEgNlnm/SS2aSzaW+4fgRRe/SSJ8J//k9mf6y6LnyaLwMhzPEQk0CUnLo+ObXm4oLSu/Yv4KXfU3UU69V/RCsRAo7oWzdfQTIqHNfZuedazGtPFMkIECc0SzWuFYOEFr9NSoC1HO7SjjYjOR4RXvOj9PSLmZtcXNBb8O5vgVyrAIkuIjIEXOIDlM7J2E+izlwJbNdFpEqjbBKhWapxrRgk1PhtxhRcmEpKqqQSiuOBdROXUhJxfUwuKFZE9+OY3avDagW5CpBop0LALVaJPqlCVXGd5Txm4vKkSgVcDtyp1czkGuZtyVti1pZ//JdV5ejhrqI4ntNws6kTXYZCgws6VMHxVcKBSa8CJFufESHgeE31wVEFl1+6H8KpYfFvWapYJGjWaoRmqaDlViztcsiCfZpEcTysdBZQUc6fsbIgV+ZqSGMDKHr1H2RA1qs8Ycx56esMuClk4BEnSx24Wo3QzCrPIsIlZUV5/bjrKo5HwWUeiD9jUDpfltP/mEoFXL36j2wN5gAh4HhdDZZ9vGHrEkz9CWJewQ4Qx7P24muFGq2Zq7eQuIhHtmxVvepIjgdKo0XRnTH9n2E3AFrMMxS9+g/nQJK77t4h83DgwMVp4jsyAsTxYLXV6KlvVtBCEuKOsOg5dkkGdgPc1CWUYnNppfNlu58muDIEnDgFtHDgLud5+t0IEHc+cAfE/VaMiKaTUhwP4MZPVx8qu+DKb3FSDAa9+o9UwKNzh825Aq74BnALGceV5goQZw8GqNGGaALhzohwXEW5ABjCY8381/VG8eNIRxdYaSTz8U9zgVaKsStCGWID6+pVgERTFQLuQVxZLJiczMmxm7oIV5orQBxr1mq0ZtnKd5uycgj9yRt8UVvohMnsPoG8bIWVDnExRCsFXOaC+G4FuHr1H1FFhYC7l+97wIcWyrqZ7X8m1lnCTSOzAOecmLjRajVas9Dhv8nguYjFu0SFySKxq4f9mFiuCI7nFPq9VWrQSudLFMHrXIk3Ai63r8lVgLiSDgG3VBYrAc1RHAMGdXDXm3VKBzS9pNVozVyxlSyPfvFL8vWWuFWhE3BFsh/CiuMBARSfsKznSm4u6HU1ur2A0TKOr0WtAsT96BBwiV+jeJ5pLYxYriPfeNClDr2k1GiWiSu2kswNE/vuFRUiVo0mh8UKf7KB5HiYBEp85Gtf3T3awAXdfGUexnWtAsTtdAi47/zkN3HEUeFkHRqqGaWaXqKkUKNZJlbQpgxhEnx1zbPyTzh3vuJZtlGZUVw4DW89KShp+P9h3HmfykbBzW6Bh13Vq/fBYc6N9Rbc5Db6ps/TTbxv+lTJ61Q2Lu+RvR+Sd7QmCD94My6a9zdlb3RGnx8Rff/U1BW1jUbn7j9VoGXcdjeL4KkzV98sYDkac2KkunngPnJ6BldUKKEFQguEFviltcDLoHY2YomgltRO6y/pSf6b2UL8GCAfwqiFJqPIh9oxaohkrjEDxzMeeSorh70HA4T/0JjeqvP2PAV/XtsRr9/wLeBydQ8GKMtdeRBG7SCQddqD60np+MP1rI7+nm0fl0/N9uB6Ujr+cD2rA8B9PiDaz24Priel4w/XszowHWsfl0/N9uB6Ujr+cD2rY6rL508YQ2tkfgRcQdAMnFw7fPcP+dhF7TS1cF6nlR5DlLjzFjSy6ZyZ+MHbSqyGRZaRWEfIdjCK7NkzS4Mj22SpQxgRPT51OabWbCDUxPywzoigb9JLP8vTiRKmzT5A8Q/bd4amFmZ0GnFneK14ndqmc2YOU1LNSBLJMvqsWEcoytWZARrgcsyrqdJ9OBKEEWYLBp8nGwjubo3MD2pqgkYHe3NROx4tHuM2QkT/iap8G3wmJ+kcvecaqkytIySq8305vRVl39Clcha7v070JmR/BTOLz4m5Tkx2eDI/iqDpx19NjFk3tePdAmpZRP/ZqxF1qaqIImSatI4qW5nFLNaarM5TsnQSnzeTbsnW5bnYOGZEKZqzo86RN/OzLugbXrqdg725qR2jxdCeIus1RMLFWWIKWBJFNWfP9bKybLLE6wgZcJeQkdOlNkcwwNNd2Cgg+EPJF2ET8/M9SdD0jwu4bmrHaJGeXkNrUwTcTAFwSw5RZNI6qkzMmqsvQ1i3D4O0TLqltm7fMJQnLgZTIIBgnp/HeDPzI+gbHYHKRe24WmBK2C1yMNL9B+dKzmS7e65ZlE3URDtZnQcDvsQBzpItNVzhppPYqoGQD/OjCJr+otDjgmu2kEG2uBctsv8D1wvraph6LyrJsia4uNbeiXLV0g+uD/PDYxdmV3Bd1I7ZQgTZ0kjFHnBjcgSWvK0rxm5JrSPEY1dyFDxyMG2gSnns5vlSE4MhhsFQlIPBxfy89ej75HekCRpVy0XtmC2Gtn/9vbe64dZBoCzleewmbebFtK4qm8TZQKJcXYxd3HRzuPPmZUtNGAnyExsN12R+BrdVn5plJWSs4MODykXtmC36RvKJUdFCb6A3xqss0WTp+55wVdlKHS1wg+LqEu7KH5XF+ky6pbCu8CrCTU/DNZmfpQv0L2JN3+haizD6iWF0ADFbnNhKqRHkOY8J3KWPCJZ7buGIJ9yxMkAtHFHrCInqcgmhge1QpErFMkKCMNqNnu+o2XDJYX4ymOm/o4Q2Dn3DlA1/Hya1Y7TgxYEiYDSMh7CgdJ6oU+Lts4/aTkN7oFLduUiX3SHWEXIYIEryc02X2oRR5AUavBGDRgEhN/PDWFkkfTM0MVKPTI88T/Tw5Z/PXvOCLHNa0Cq+JhCHzk8cIqZ04lcevDf+i5Kmc/ReNFdllJbrCNkMEBGr16UOYfTwK36K55sNRELgLTM/5yYZ3HaX+S2SjZA+k2BtCwDfdifHD7dVt+OVhnjwLJyLWh63E/ld59Kkc3XT22r0SO5c9PG4nfiLm86lSQfrvuP00deck7qBq/FSwb/hVtMt+T8QqOaFwDZ99wAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=attachment:CodeCogsEqn%283%29.png width=\"230\" align=\"left\">\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3> Task: Implement LSTM </h3>\n",
    "    Now, it is your turn. Implement your own LSTM with the operations stated above. Go to <code>exercise_code/rnn/rnn_nn.py</code> and complete the implementation of the <code>LSTM</code> class.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done with the task, the following cell will check whether your implementation has been correct. Similar to the RNN, Pytorch of course also provides an implementation for a LSTM which can be called with `nn.LSTM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the LSTM class\n",
    "from exercise_code.rnn.rnn_nn import LSTM\n",
    "\n",
    "# choose your input parameters\n",
    "input_size=3\n",
    "hidden_dim=3\n",
    "seq_len= 10 \n",
    "\n",
    "# define the two models\n",
    "pytorch_lstm = nn.LSTM(input_size, hidden_dim)\n",
    "i2dl_lstm = LSTM(input_size, hidden_dim)\n",
    "\n",
    "x = torch.randn(seq_len, 1, input_size)\n",
    "rnn_output_test(i2dl_lstm, pytorch_lstm, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, if the test above has been successful, you implemented your LSTM correctly. Of course, this task is meant to pratice your understanding on the LSTM architecture. In future,  you can better use the Pytorch implementation as this will probably be faster and optimized in performance. Let us check that out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "runs=10**4\n",
    "\n",
    "print(\"Time Pytorch LSTM {} runs: {:.3f}s\".format(runs, timeit.timeit(\"pytorch_lstm(x)\", \n",
    "                                       setup=\"from __main__ import pytorch_lstm, x\", \n",
    "                                       number=runs))\n",
    "     )\n",
    "\n",
    "print(\"Time I2DL LSTM {} runs: {:.3f}s\".format(runs, timeit.timeit(\"i2dl_lstm(x)\", \n",
    "                                       setup=\"from __main__ import i2dl_lstm, x\", \n",
    "                                       number=runs))\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Explore Gradients\n",
    "Analogously to the RNN, calculate the gradients of the input wrt. to the output of the LSTM and compare it against the RNN gradients. __What do you see?__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 1\n",
    "input_size = 1\n",
    "\n",
    "time_steps=50\n",
    "rnn = RNN(input_size, hidden_size)\n",
    "for p in rnn.parameters():\n",
    "    p.data.fill_(0.1)\n",
    "X = torch.randn(time_steps, 1, input_size)\n",
    "X.requires_grad = True\n",
    "_,h = rnn(X)\n",
    "h.requires_grad\n",
    "h.sum().backward()\n",
    "grad_rnn = X.grad.view(-1)\n",
    "\n",
    "lstm = LSTM(input_size, hidden_size)\n",
    "for p in lstm.parameters():\n",
    "    p.data.fill_(0.1)\n",
    "X = torch.randn(time_steps, 1, input_size)\n",
    "X.requires_grad=True\n",
    "_,(h, c) = lstm(X)\n",
    "h.sum().backward()\n",
    "grad_lstm = X.grad.view(-1)\n",
    "\n",
    "plt.semilogy(np.flip(abs(grad_lstm.detach().cpu().numpy())) , label=\"LSTM\")\n",
    "plt.semilogy(np.flip(abs(grad_rnn.detach().cpu().numpy())), label=\"RNN\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time step t\")\n",
    "plt.ylabel(\"d h_T/d x_t\")\n",
    "plt.title(\"Log plot of gradient of output wrt. input\")\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploding Gradients Problem\n",
    "\n",
    "In the previous sections, we addressed the vanishing gradients problem by switching from vanilla RNNs to LSTMs.\n",
    "\n",
    "The vanishing gradient problem occurs when weights are smaller 1. When weights are bigger than 1, we have another problem called the <b>exploding gradient</b>, where gradients end up getting very large values. This can be observed best with the `relu` RNN cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 1\n",
    "input_size = 1\n",
    "\n",
    "time_steps=50\n",
    "rnn = RNN(input_size, hidden_size, activation='relu')\n",
    "for p in rnn.parameters():\n",
    "    p.data.fill_(1.3)\n",
    "X = torch.randn(time_steps, 1, input_size)\n",
    "X.requires_grad = True\n",
    "_,h = rnn(X)\n",
    "h.requires_grad\n",
    "h.sum().backward()\n",
    "grad_rnn = X.grad.view(-1)  # next(rnn.parameters()).grad.view(-1)\n",
    "\n",
    "plt.semilogy(np.flip(abs(grad_rnn.detach().cpu().numpy())), label=\"RNN\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time step t\")\n",
    "plt.ylabel(\"d h_T/d x_t\")\n",
    "plt.title(\"Log plot of gradient of output wrt. input\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One possible solution of this problem is gradient clipping. Gradient clipping shrinks gradient magnitude. We can use [clip_grad_norm_](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html) function from PyTorch here. Observe the gradient magnitude is smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "grad_rnn_unclipped = grad_rnn.clone()\n",
    "\n",
    "hidden_size = 1\n",
    "input_size = 1\n",
    "gclip = 40\n",
    "\n",
    "time_steps=50\n",
    "rnn = RNN(input_size, hidden_size, activation='relu')\n",
    "for p in rnn.parameters():\n",
    "    p.data.fill_(1.5)\n",
    "X = torch.randn(time_steps, 1, input_size)\n",
    "X.requires_grad = True\n",
    "_,h = rnn(X)\n",
    "h.requires_grad\n",
    "h.sum().backward()\n",
    "clip_grad_norm_(X, gclip)\n",
    "grad_rnn = X.grad.view(-1)  # next(rnn.parameters()).grad.view(-1)\n",
    "\n",
    "plt.semilogy(np.flip(abs(grad_rnn_unclipped.detach().cpu().numpy())), label=\"Not Clipped\")\n",
    "plt.semilogy(np.flip(abs(grad_rnn.detach().cpu().numpy())), label=\"Clipped\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time step t\")\n",
    "plt.ylabel(\"d h_T/d x_t\")\n",
    "plt.title(\"Log plot of gradient of output wrt. input\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Handling Batches with Different Sequence Lenghts\n",
    "\n",
    "Sequences used in natural language processing tasks usually have different lengths. Unlike images, we cannot guarantee sequences to have the same shape. Therefore, we need to use <b>padding</b> and <b>packing</b>. PyTorch provides some operations for that. Before moving forward, see the documentations of [pack_padded_sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html) and [pad_packed_sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html) functions.\n",
    "\n",
    "In this section, you don't have to implement anything. You should just understand how we handled the mini-batches with varying length sequences.\n",
    "\n",
    "Let's import the relevant functions first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import (\n",
    "    pack_padded_sequence,\n",
    "    pad_packed_sequence\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We represent a padded sequence with a tensor of size `(max_len, batch_size, feature_size)` and a 1d lengths tensor of size `batch_size`. \n",
    "\n",
    "As an example, tensors below mean we have 2 sequences of lengths 3 and 2. Therefore, the elements `padded_seq[2, 1, :]` are paddings whose values we should ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_seq = torch.arange(0, 12).view(3, 2, 2).float()\n",
    "lengths = torch.tensor([3, 2]).long()\n",
    "\n",
    "padded_seq, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch RNNs internally use the [PackedSequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html) to store and process batches with varying length sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed_seq = pack_padded_sequence(padded_seq, lengths)\n",
    "packed_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PackedSequence` is an internal representation PyTorch uses, about whom you can learn more about reading the PyTorch documentation. The packing can be inverted as follows, zeroing out the padded values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_packed_sequence(packed_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "<p>For convenience, we use padded representation in our RNN implementations and then ignore padded output. Check the <code>forward</code> functions of <code>RNN</code> and <code>LSTM</code> classes in <code>exercise_code/rnn/rnn_nn.py</code>. </p>\n",
    "    \n",
    "<p>For packed inputs, we switched to padded representation using <code>pad_packed_sequence</code>. In the end, we used the <code>pack_outputs</code> function to remain consistent with PyTorch.</p>\n",
    "</div>\n",
    "\n",
    "You can see that our implementation behaves identically to PyTorch RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_lstm = nn.LSTM(2, 2)\n",
    "i2dl_lstm = LSTM(2, 2)\n",
    "\n",
    "rnn_output_test(i2dl_lstm, torch_lstm, packed_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, you completed the optional notebook on RNNs and LSTMs. Now, you should be well prepared for this week's exercise on Sentiment Analysis! Check out the other two notebooks and have fun!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
