{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing and Word Embeddings\n",
    "\n",
    "Welcome to this new exercise! In this exercise, we will play around with text instead of images as before, using Recurrent Neural Networks. Generally, it is called Natural Language Processing (NLP) when dealing with text, speech, etc. But the data structure is very different from images, i.e., text is a string, while images consist of numbers. Hence, we need some preprocessing steps to transform the raw text into another data format. This notebook will introduce these basic concepts in NLP pipelines. Specifically, you will learn about:\n",
    "\n",
    "1. How to preprocess text classification datasets\n",
    "2. How to create a simple word embedding layer that maps words to dense vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Mount folder in Colab\n",
    "\n",
    "Uncomment the following cell to mount your gdrive if you are using the notebook in google colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following lines if you want to use Google Colab\n",
    "# We presume you created a folder \"i2dl\" within your main drive folder, and put the exercise there.\n",
    "# NOTE: terminate all other colab sessions that use GPU!\n",
    "# NOTE 2: Make sure the correct exercise folder (e.g exercise_11) is given.\n",
    "\n",
    "\"\"\"\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "gdrive_path='/content/gdrive/MyDrive/i2dl/exercise_11'\n",
    "\n",
    "# This will mount your google drive under 'MyDrive'\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "# In order to access the files in this notebook we have to navigate to the correct folder\n",
    "os.chdir(gdrive_path)\n",
    "# Check manually if all files are present\n",
    "print(sorted(os.listdir()))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up PyTorch environment in colab\n",
    "- (OPTIONAL) Enable GPU via Runtime --> Change runtime type --> GPU\n",
    "- Uncomment the following cell if you are using the notebook in google colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install correct libraries in google colab\n",
    "# !python -m pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# !python -m pip install tensorboard==2.8.0\n",
    "# !python -m pip install pytorch-lightning==1.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup\n",
    "\n",
    "As usual, we first import some packages to setup this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\xshys\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\users\\xshys\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\users\\xshys\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from exercise_code.rnn.sentiment_dataset import (\n",
    "    create_dummy_data,\n",
    "    download_data\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing a Text Classification Dataset\n",
    "\n",
    "As a starting point, let's load a dummy text classification dataset and have a sense of how it looks. We take these samples from the IMDb movie review dataset, which includes movie reviews and labels that show whether they are negative (0) or positive (1). You will investigate this task further in the second notebook.\n",
    "\n",
    "In this section, our goal is to create a text processing dataset. You are not required to write any code in this section. However, the concept introduced here is very important for working on NLP datasets in the future as well as in the rest of this exercise. \n",
    "Take your time to understand the procedure here. \n",
    "\n",
    "First, let us download the data and take a look at some data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "data_root = os.path.join(i2dl_exercises_path, \"datasets\", \"SentimentData\")\n",
    "path = download_data(data_root)\n",
    "data = create_dummy_data(path)\n",
    "for text, label in data:\n",
    "    print('Text: {}'.format(text))\n",
    "    print('Label: {}'.format(label))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Tokenizing Data\n",
    "\n",
    "As seen above, we loaded 3 positive and 3 negative reviews. Since the basic semantic unit of text is a word, the first thing we need to do is **tokenizing** the dataset, which means converting each review to a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# use regular expression to split the sentence\n",
    "# check https://docs.python.org/3/library/re.html for more information\n",
    "def tokenize(text):\n",
    "    return [s.lower() for s in re.split(r'\\W+', text) if len(s) > 0]\n",
    "\n",
    "tokenized_data = []\n",
    "for text, label in data:\n",
    "    tokenized_data.append((tokenize(text), label))\n",
    "    print(tokenized_data[-1], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Creating a Vocabulary\n",
    "\n",
    "We have converted the dataset into pairs of token lists and corresponding labels. But strings have varying lengths, which is hard to handle. It would be nice to represent words with numbers. So, we need to create a <b>vocabulary</b>, which is a dictionary that maps each word to an integer id.\n",
    "\n",
    "In large datasets, there are too many words, and most of them don't occur very frequently. One common approach we use to tackle this problem is to pick the most common N words from the dataset. Therefore, we restrict the number of words.\n",
    "\n",
    "First, let's compute the word frequencies in our dummy dataset. To compute frequencies, we use the [Counter](https://docs.python.org/3/library/collections.html#collections.Counter) data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "freqs = Counter()\n",
    "for tokens, _ in tokenized_data:\n",
    "    freqs.update(tokens)\n",
    "\n",
    "freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the dictionary, let's select the most common 20 words to create a vocabulary. In addition to the words that appear in our data, we need to have two special words:\n",
    "\n",
    "- `<eos>` End of sequence symbol used for padding\n",
    "- `<unk>` Words unknown in our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {'<eos>': 0, '<unk>': 1}\n",
    "for token, freq in freqs.most_common(20):\n",
    "    vocab[token] = len(vocab)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Creating the Dataset\n",
    "\n",
    "Putting it all together, we can now create a dataset class. First, let's create index-label pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_data = []\n",
    "for tokens, label in tokenized_data:\n",
    "    indices = [vocab.get(token, vocab['<unk>']) for token in tokens]    \n",
    "    # the token that is not in vocab get assigned <unk>\n",
    "    indexed_data.append((indices, label))\n",
    "    \n",
    "\n",
    "for indices, label in indexed_data:\n",
    "    print(indices, ' -> ', label)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"> \n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>We now use the PyTorch dataset class we provided in <code>exercise_code/rnn/sentiment_dataset.py</code> file. Please also take a look at the code.</p>\n",
    " </div>\n",
    "    \n",
    "\n",
    "\n",
    "Dataset class also reverse sorts the sequences with respect to the lengths. Thanks to this sorting, we can reduce the total number of padded elements, which means that we have less computations for padded values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.rnn.sentiment_dataset import SentimentDataset\n",
    "\n",
    "combined_data = [\n",
    "    (raw_text, tokens, indices, label)\n",
    "    for (raw_text, label), (tokens, _), (indices, _)\n",
    "    in zip(data, tokenized_data, indexed_data)\n",
    "]\n",
    "\n",
    "dataset = SentimentDataset(combined_data)\n",
    "\n",
    "for elem in dataset:\n",
    "    print(elem)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Minibatching\n",
    "Note that in the dataset we created, not all sequences have the same length. Therefore, we cannot minibatch the data trivially. This means we cannot use a `DataLoader` class easily.\n",
    "\n",
    "<b>If you uncomment the following cell and run it, you will very likely get an error!</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = DataLoader(dataset, batch_size=3)\n",
    "\n",
    "# for batch in loader:\n",
    "#     print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"> \n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>To solve the problem, we need to pad the sequences with <code> < eos > </code> tokens that we indexed as zero. To integrate this approach into the Pytorch <a href=\"https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\" target=\"_blank\">Dataloader</a> class, we will make use of the <code>collate_fn</code> argument. For more details, check out the <code>collate</code> function in <code>exercise_code/rnn/sentiment_dataset</code>. </p>\n",
    "    <p> In addition, we use the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html\" target=\"_blank\">pad_sequence</a> that pads shorter sequences with 0. </p>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate(batch):\n",
    "    assert isinstance(batch, list)\n",
    "    data = pad_sequence([b['data'] for b in batch])\n",
    "    lengths = torch.tensor([len(b['data']) for b in batch])\n",
    "    label = torch.stack([b['label'] for b in batch])\n",
    "    return {\n",
    "        'data': data,\n",
    "        'label': label,\n",
    "        'lengths': lengths\n",
    "    }\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=3, collate_fn=collate)\n",
    "for batch in loader:\n",
    "    print('Data: \\n', batch['data'])\n",
    "    print('\\nLabels: \\n', batch['label'])\n",
    "    print('\\nSequence Lengths: \\n', batch['lengths'])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that these two batches have different length, this is how the reverse sort mentioned in `1.3 Creating the Dataset` benefits for less memory and less computation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Embeddings\n",
    "\n",
    "In the previous section, we explored how to convert text into a sequence of integers. In this form, sequences are still not ready to be inputs of RNNs you implemented in the optional notebook. \n",
    "\n",
    "An integer representation is usually a one-hot encoding, while not the same since they are not equally weighted given only an integer. \n",
    "\n",
    "Moreover, it fails to express the semantic relations between words and the order of the words has no meaning. We would like a better representation to keep the semantic meaning of the word. For example, as shown in the following picture, the difference between man and woman and the difference between king and queen should be close, since the difference is only the gender. If we use a vector for each word, the above relation can be expressed as $vec(\\text{women})-vec(\\text{man}) \\approx vec(\\text{queen}) - vec(\\text{king})$. Usually we call such vector representations as embeddings.\n",
    "\n",
    "<img src='https://developers.google.com/machine-learning/crash-course/images/linear-relationships.svg' width=80% height=80%/>\n",
    "\n",
    "While one can use pre-trained embedding vectors such as [word2vec](https://arxiv.org/abs/1301.3781) or [GLoVe](https://nlp.stanford.edu/projects/glove/), in this exercise we use randomly initialized embedding vectors that will be trained from scratch together with our networks. As we train our model, it will learn the semantic relations between words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<h3> Task: Implement Embedding</h3>\n",
    " <p>In this part, you will implement a simple embedding layer. Embedding is a simple lookup table that stores a dense vector to represent each word in the vocabulary.</p> \n",
    "\n",
    " <p>Your task is to implement the <code>Embedding</code> class in <code>exercise_code.rnn.rnn_nn</code> file. Once you are done, run the below cell to test your implementation. Note that we ensure eos embeddings to be zero by using the <code>padding_idx</code> argument.\n",
    "\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "from exercise_code.rnn.rnn_nn import Embedding\n",
    "from exercise_code.rnn.tests import embedding_output_test\n",
    "\n",
    "\n",
    "i2dl_embedding = Embedding(len(vocab), 16, padding_idx=0)\n",
    "pytorch_embedding = nn.Embedding(len(vocab), 16, padding_idx=0)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=len(dataset), collate_fn=collate)\n",
    "for batch in loader:\n",
    "    x = batch['data']\n",
    "\n",
    "embedding_output_test(i2dl_embedding, pytorch_embedding, x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Conclusion\n",
    "\n",
    "In this notebook, you learned how to prepare text data and how to create an embedding layer. In the next notebook, you will combine your Embedding and RNN implementations to create a sentiment analysis network!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
